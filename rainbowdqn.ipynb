{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"LDrzl80g8N5v","jupyter":{},"scrolled":false,"tags":[],"colab":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"import sys\n#sys.path.append('/content/drive/My Drive/DQN/input/')\nsys.path.append('/home/kesci/input')\nimport math, random\nimport gym\nimport numpy as np\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom common8945.wrappers import make_atari, wrap_deepmind, wrap_pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torchvision.transforms as T","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gtR9C8hF8U45","jupyter":{},"scrolled":false,"tags":[],"colab":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"def plot(frame_idx, rewards):\n    clear_output(True)\n    plt.figure(figsize=(20,5))\n    plt.subplot(131)\n    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n    plt.plot(rewards)\n    plt.show()\n    \ndef plot2(frame_idx, rewards, losses, reward_episodes, loss_frames):\n    clear_output(True)\n    plt.figure(figsize=(20,5))\n    plt.subplot(131)\n    plt.title('frame %s. average reward for every %s episode(s): %s' % (frame_idx, reward_episodes, np.mean(rewards[-10:])))\n    plt.plot(rewards)\n    plt.subplot(132)\n    plt.title('frame %s. average loss for every %s frame(s)' % (frame_idx, loss_frames))\n    plt.plot(losses)\n    plt.show()","execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6B905CFC5671425B8DE5C9BD1B7C9340","jupyter":{},"scrolled":false,"tags":[],"outputId":"01af5619-eedf-48b5-9a8e-f28bf042fd61","colab":{"base_uri":"https://localhost:8080/","height":69},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"#hyper-parameters\nDEV = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nEPSILON_BEGIN = 0.99\nEPSILON_END = 0.01\nEPSILON_TOTAL = 1000000  \nBATCH_SIZE = 32\nNO_OP = 0\nLEARNING_RATE = 0.0000625\nGAMMA = 0.99\nMEMORY_CAPACITY = 2 ** 15\nATOMS_NUM = 51\nVMIN = -10.0\nVMAX = 10.0\nDELTA = 1.0 * (VMAX - VMIN) / (ATOMS_NUM - 1)\nTARGET_UPDATE = 1000\nEPISODE = 100000\nLEARNING_START = MEMORY_CAPACITY + 10000\nPRINT_START = MEMORY_CAPACITY + 20000\nPROBABILITY_MIN = 0.01\nPROBABILITY_MAX = 0.99\nPRIORITIZATION_EPS = 0.0001\nPRIORITIZATION_ALPHA = 0.6\nPRIORITIZATION_BETA = 0.4\nPRIORITIZATION_DELTA_BETA = 0.00000003","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aEfTeOW88dwU","jupyter":{},"scrolled":false,"tags":[],"colab":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"class SegmentTree(object):\n  \n  def __init__(self, capacity):\n    self.capacity = capacity\n    self.sum = [0.0 for i in range (capacity << 1)]\n    self.min_ = [1.0 for i in range (capacity << 1)]\n\n  def update(self, index, val):\n    index += self.capacity\n    self.sum[index] = val ** PRIORITIZATION_ALPHA\n    self.min_[index] = val ** PRIORITIZATION_ALPHA\n    while index != 1:\n      index >>= 1\n      self.sum[index] = self.sum[index << 1] + self.sum[index << 1 | 1]\n      self.min_[index] = min(self.min_[index << 1], self.min_[index << 1 | 1])\n  \n  def query(self, val):\n    rt = 1\n    while rt < self.capacity:\n      if val < self.sum[rt << 1]:\n        rt = rt << 1\n      else: \n        val -= self.sum[rt << 1]\n        rt = rt << 1 | 1 \n    return rt - self.capacity, self.sum[rt]\n  \n  def query_sum(self):\n    return self.sum[1]\n\n  def query_min(self):\n    return self.min_[1]\n\n\nclass PrioritizedMemory(object):\n\n    def __init__(self):\n        self.tree = SegmentTree(MEMORY_CAPACITY)\n        self.memory = []\n        self.position = 0\n        self.max_priority = 1.0\n\n    def clamp_error(self, error):\n        if error > 1:\n          return 1\n        if error < 0:\n          return 0\n        return error\n\n    def push(self, state, action, reward, next_state, done):\n        if len(self.memory) < MEMORY_CAPACITY:\n            self.memory.append(None)\n        self.tree.update(self.position, self.max_priority)\n        self.memory[self.position] = (state, action, reward, next_state, done)\n        self.position = (self.position + 1) % MEMORY_CAPACITY\n\n    def sample(self, batch_size, beta):\n        indexes, weights, states, actions, rewards, next_states, dones = [], [], [], [], [], [], []\n        average = self.tree.query_sum() / batch_size\n        min_priority = self.tree.query_min()\n        sum_priority = self.tree.query_sum()\n        min_priority = min_priority / sum_priority\n        max_weight = (min_priority * MEMORY_CAPACITY) ** (-beta)\n        for i in range(batch_size):\n          l = average * i\n          r = average * (i + 1)\n          val = random.uniform(l, r) #[l, r)\n          index, priority = self.tree.query(val)\n          state, action, reward, next_state, done = self.memory[index]\n          indexes.append(index)\n          states.append(state)\n          actions.append(action)\n          rewards.append(reward)\n          next_states.append(next_state)\n          dones.append(done)\n          priority = priority / sum_priority\n          weight = (priority * MEMORY_CAPACITY) ** (-beta)\n          weights.append(weight / max_weight)\n        return indexes, weights, states, actions, rewards, next_states, dones\n\n    def update_errors(self, indexes, priorities):\n      for i in range(BATCH_SIZE):\n        self.tree.update(indexes[i], priorities[i] + PRIORITIZATION_EPS)\n        self.max_priority = max(self.max_priority, priorities[i])\n        \n    def size(self):\n        return len(self.memory)","execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YkbVyksb9wnM","colab":{},"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"class FactorisedGaussianNoise(nn.Module):\n  def __init__(self, input, output, init = 0.5):\n    super(FactorisedGaussianNoise, self).__init__()\n    self.input = input\n    self.output = output\n    self.mu_w = nn.Parameter(torch.FloatTensor(output, input))\n    self.sigma_w = nn.Parameter(torch.FloatTensor(output, input))\n    self.register_buffer('eps_w', torch.FloatTensor(output, input))\n    self.mu_b = nn.Parameter(torch.FloatTensor(output))\n    self.sigma_b = nn.Parameter(torch.FloatTensor(output))\n    self.register_buffer('eps_b', torch.FloatTensor(output))\n    \n    #initialize parameters\n    range = 1.0 / math.sqrt(input)\n    self.mu_w.data.uniform_(-range, range)\n    self.sigma_w.data.fill_(init / math.sqrt(input))\n    self.mu_b.data.uniform_(-range, range)\n    self.sigma_b.data.fill_(init / math.sqrt(output))\n    self.reset()\n\n  def forward(self, x):\n    if self.training:\n      return F.linear(x, self.mu_w + self.sigma_w * self.eps_w, self.mu_b + self.sigma_b * self.eps_b)\n    else:\n      return F.linear(x, self.mu_w, self.mu_b)\n      \n  def reset(self):\n    eps_input = self.random_noise(self.input)\n    eps_output = self.random_noise(self.output)\n    self.eps_w.copy_(eps_output.ger(eps_input))\n    self.eps_b.copy_(eps_output)\n\n  def random_noise(self, size):\n    res = torch.randn(size)\n    return res.sign() * res.abs().sqrt()","execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"30922C4C798E479880E2B463D33E2CFF","jupyter":{},"scrolled":false,"tags":[],"colab":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"class RainbowDQN(nn.Module):\n\n    def __init__(self, input_size, atoms_num, action_space):\n        super(RainbowDQN, self).__init__()\n        \n        self.atoms_num = atoms_num\n        self.action_space = action_space\n        \n        self.conv1 = nn.Conv2d(input_size, 32, kernel_size = 8, stride = 4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n        \n        self.fc_value = FactorisedGaussianNoise(7 * 7 * 64, 512)\n        self.output_value = FactorisedGaussianNoise(512, atoms_num)\n        \n        self.fc_advantage = FactorisedGaussianNoise(7 * 7 * 64, 512)\n        self.output_advantage = FactorisedGaussianNoise(512, atoms_num * action_space)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n\n        x_value = F.relu(self.fc_value(x.view(x.size(0), -1)))\n        x_value = self.output_value(x_value)\n        x_value = x_value.view(x.size(0), 1, self.atoms_num)\n        \n        x_advantage = F.relu(self.fc_advantage(x.view(x.size(0), -1)))\n        x_advantage = self.output_advantage(x_advantage)\n        x_advantage = x_advantage.view(x.size(0), self.action_space, self.atoms_num)\n        \n        x = x_value + x_advantage - x_advantage.mean(1, keepdim = True)\n        x = F.softmax(x.view(-1, self.atoms_num), dim = 1).view(-1, self.action_space, self.atoms_num)\n        \n        return x   \n    \n    def reset(self):\n      self.fc_value.reset()\n      self.output_value.reset()\n      self.fc_advantage.reset()\n      self.output_advantage.reset()\n\n    def evaluate(self):\n      self.eval()\n      self.fc_value.eval()\n      self.output_value.eval()\n      self.fc_advantage.eval()\n      self.output_advantage.eval()","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"08AC28B2E556470F9747FA6813F71F35","jupyter":{},"scrolled":false,"tags":[],"colab":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false},"source":"class Agent: \n    def __init__(self, action_space, model_path = None):\n        self.action_space = action_space\n        self.memory = PrioritizedMemory()\n        self.policy_net = RainbowDQN(1, ATOMS_NUM, self.action_space).to(DEV)\n        self.target_net = RainbowDQN(1, ATOMS_NUM, self.action_space).to(DEV)\n        if model_path == None:\n            self.play_mode = False\n        else:\n            self.paly_mode = True\n            self.policy_net = torch.load(model_path) #,  map_location=torch.device('cpu'))\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        #self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr = EARNING_RATE, eps = 0.001, alpha = 0.95)\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr = LEARNING_RATE)#, eps = 0.000015)\n        \n    def take_action(self, state):\n        with torch.no_grad():\n          output = self.policy_net(state)\n          z = torch.linspace(VMIN, VMAX, ATOMS_NUM, dtype = torch.float, device = DEV)\n          output = output * z\n          return output.sum(2).max(1)[1]\n    \n    def act(self, frame):\n        with torch.no_grad():\n            state = torch.tensor(frame / 255.0, device = DEV, dtype = torch.float).unsqueeze(0) \n            output = self.policy_net(state)\n            z = torch.linspace(VMIN, VMAX, ATOMS_NUM, dtype = torch.float, device = DEV)\n            output = output * z\n            return output.sum(2).max(1)[1]\n   \n    def optimize(self, frame_idx):\n        beta = min(1.0, PRIORITIZATION_BETA + PRIORITIZATION_DELTA_BETA * frame_idx)\n       # print(PRIORITIZATION_BETA, PRIORITIZATION_DELTA_BETA, frame_idx, beta)\n        indexes, weights, states, actions, rewards, next_states, dones = self.memory.sample(BATCH_SIZE, beta)\n        batch_states = torch.cat(states).to(DEV)\n        batch_next_states = torch.cat(next_states).to(DEV)\n        batch_weights = torch.tensor(weights, device = DEV, dtype = torch.float)\n        batch_actions = torch.tensor(actions, device = DEV, dtype = torch.long)\n        batch_rewards = torch.tensor(rewards, device = DEV, dtype = torch.float) \n        batch_dones = torch.tensor(dones, device = DEV, dtype = torch.float)\n        \n        policy_output = self.policy_net(batch_states)\n        policy_action = batch_actions.unsqueeze(1).unsqueeze(1).expand(BATCH_SIZE, 1, ATOMS_NUM)\n        policy_value = policy_output.gather(1, policy_action).squeeze(1)\n        \n        z = torch.linspace(VMIN, VMAX, ATOMS_NUM, dtype = torch.float, device = DEV)\n        target_output = self.target_net(batch_next_states) * z\n        target_action = target_output.sum(2).max(1)[1]\n        target_action = target_action.unsqueeze(1).unsqueeze(1).expand(BATCH_SIZE, 1, ATOMS_NUM)\n        target_value = target_output.gather(1, target_action).squeeze(1)\n        \n        batch_dones = batch_dones.unsqueeze(1).expand(BATCH_SIZE, ATOMS_NUM)\n        batch_rewards = batch_rewards.unsqueeze(1).expand(BATCH_SIZE, ATOMS_NUM)\n        z = z.unsqueeze(0).expand(BATCH_SIZE, ATOMS_NUM)\n        Tz = batch_rewards + GAMMA * (1 - batch_dones) * z\n        Tz.clamp_(VMIN, VMAX)\n        b  = (Tz - VMIN) / DELTA\n        l  = b.floor().long()\n        u  = b.ceil().long()\n        idx = torch.linspace(0, (BATCH_SIZE - 1) * ATOMS_NUM, BATCH_SIZE, dtype = torch.float, device = DEV).long()\n        idx = idx.unsqueeze(1).expand(BATCH_SIZE, ATOMS_NUM)\n        m = torch.zeros(BATCH_SIZE * ATOMS_NUM, dtype = torch.float, device = DEV)    \n        m.index_add_(0, (l + idx).view(-1), (target_value * (u.float() - b)).view(-1))\n        m.index_add_(0, (u + idx).view(-1), (target_value * (b - l.float())).view(-1))\n        m = m.view(BATCH_SIZE, ATOMS_NUM)\n        \n        policy_value.data.clamp_min_(PROBABILITY_MIN)\n        m.data.clamp_min_(PROBABILITY_MIN)\n        loss = -(m * policy_value.log()).sum(1)\n        td_errors = loss.clone().to(DEV)\n        td_errors = td_errors.data.clamp_min_(0.0)\n        self.memory.update_errors(indexes, td_errors.detach().cpu().numpy().tolist())\n\n        loss = loss * batch_weights\n        loss = loss.mean()\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.policy_net.reset()\n        self.target_net.reset()\n\n        return loss.item()\n    \n    def store_experience(self, state, action, reward, next_state, done):\n        self.memory.push(state.to('cpu'), action.to('cpu'), reward, next_state.to('cpu'), done)","execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pxe9mw1MJyJM","colab":{},"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"#env_id = \"PongNoFrameskip-v4\"\n#train(env_id, '/home/kesci/work/0510/1/', 1, 1000)\n#train(env_id, '/content/drive/My Drive/DQN/0510/1', 5, 5000)\n# You may choose the environment from these environment setting below:\n# Pong: PongNoFrameskip-v4\n# Krull: KrullNoFrameskip-v4\n# Tutankham: TutankhamNoFrameskip-v4\n# Atlantis: AtlantisNoFrameskip-v4\n# Freeway: FreewayNoFrameskip-v4\n# Beam Rider: BeamRiderNoFrameskip-v4","execution_count":23,"outputs":[]},{"metadata":{"id":"439BF30002AE4FD68B025271205C9567","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"from itertools import count\ndirectory = '/content/drive/My Drive/DQN/0515/8/'\n    \nenv_id = \"BeamRiderNoFrameskip-v4\"\nenv    = make_atari(env_id)\nenv    = wrap_deepmind(env, clip_rewards = False)\nenv    = wrap_pytorch(env)\nagent = Agent(env.action_space.n)\n \nall_losses = []#torch.load('/content/drive/My Drive/DQN/0515/8/loss12700.pth')\nall_rewards = []#torch.load('/content/drive/My Drive/DQN/0515/8/reward12700.pth')\nframe_idx = 0\nreward_episodes = 1\nloss_frames = 1000\nsum_reward = 0\nsum_loss = 0\nsum_reward = 0\nframe_idx = 0\n\nfor episode in range(1, EPISODE):\n    episode_reward = 0\n    frame = env.reset()\n    state = torch.tensor(frame / 255.0, device = DEV, dtype = torch.float).unsqueeze(0) \n    \n    for t in count():\n        frame_idx += 1\n        agent.policy_net.reset()\n        action = agent.take_action(state)\n        next_frame, reward, done, _ = env.step(action)\n        episode_reward += reward\n        next_state = torch.tensor(next_frame / 255.0, device = DEV, dtype = torch.float).unsqueeze(0)\n        agent.store_experience(state, action, reward, next_state, done)\n        state = next_state\n        \n        loss = 0\n        if frame_idx > LEARNING_START:\n            loss = agent.optimize(frame_idx)\n            if frame_idx % TARGET_UPDATE == 0:\n                agent.target_net.load_state_dict(agent.policy_net.state_dict())\n        \n        if frame_idx > PRINT_START:\n            sum_loss += loss\n            if frame_idx % loss_frames == 0:\n                sum_loss = sum_loss / loss_frames\n                all_losses.append(sum_loss)\n                sum_loss = 0\n        \n        if done:\n            break\n    \n    sum_reward += episode_reward\n    if episode % reward_episodes == 0:\n        sum_reward /= reward_episodes\n        all_rewards.append(sum_reward)\n        plot2(frame_idx, all_rewards, all_losses, reward_episodes, loss_frames)\n        sum_reward = 0\n        \n    if episode % 100 == 0 or episode_reward >= 10000:\n        torch.save(agent.policy_net, directory + 'network' + str(episode) + '.pth')\n        torch.save(all_rewards, directory + 'reward' + str(episode) + '.pth')\n        torch.save(all_losses, directory + 'loss' + str(episode) + '.pth')\n    \nenv.close()","execution_count":null}]}